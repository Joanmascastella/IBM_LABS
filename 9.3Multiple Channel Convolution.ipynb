{"cells":[{"cell_type":"markdown","id":"548dff56-5591-477f-b98e-71fdf3174db6","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","\n","\n","\n","<h1>Multiple Input and Output Channels</h1> \n"]},{"cell_type":"markdown","id":"8d004245-9b45-4538-83e3-2888f0d8b08b","metadata":{},"outputs":[],"source":["\n","<h3>Objective for this Notebook<h3>    \n","<h5> 1. Learn on Multiple Input and Multiple Output Channels.</h5>    \n","\n"]},{"cell_type":"markdown","id":"25b975cc-22af-4994-bcde-c0de455f21fe","metadata":{},"outputs":[],"source":["\n","# Table of Contents\n","In this lab, you will study convolution and review how the different operations change the relationship between input and output.\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","<li><a href=\"#ref0\">Multiple Output Channels </a></li>\n","\n","<li><a href=\"#ref1\">Multiple Inputs</a></li>\n","<li><a href=\"#ref2\">Multiple Input and Multiple Output Channels </a></li>\n","<li><a href=\"#ref3\">Practice Questions </a></li>\n","\n","<br>\n","<p></p>\n","Estimated Time Needed: <strong>25 min</strong>\n","</div>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"88e8b9f1-3a5e-45cc-829f-ce4e4c61b177","metadata":{},"outputs":[],"source":["Import the following libraries:\n"]},{"cell_type":"code","id":"6f541946-c6b9-457d-a83e-2ece7042b6dd","metadata":{},"outputs":[],"source":["import torch \nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import ndimage, misc"]},{"cell_type":"markdown","id":"7782c38d-0fba-4610-9b3d-c3b3e6dcf920","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"ca3bc827-2774-4f4a-becf-6dde212c0957","metadata":{},"outputs":[],"source":["<a id=\"ref0\"></a>\n","<h2 align=center>Multiple Output Channels </h2>\n"]},{"cell_type":"markdown","id":"b4fd4a3b-bfcf-45be-971f-5342d798d327","metadata":{},"outputs":[],"source":["In Pytroch, you can create a <code>Conv2d</code> object with multiple outputs. For each channel, a kernel is created, and each kernel performs a convolution independently. As a result, the number of outputs is equal to the number of channels. This is demonstrated in the following figure. The number 9 is convolved with three kernels: each of a different color. There are three different activation maps represented by the different colors.\n"]},{"cell_type":"markdown","id":"ee4ac247-cbf1-43c5-b146-f647ebfc1b5f","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2activationmaps.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"e8a5abf4-61c1-4b51-bcd4-225624b44eb4","metadata":{},"outputs":[],"source":["Symbolically, this can be represented as follows:\n"]},{"cell_type":"markdown","id":"4c612b54-3be3-4f66-9455-426a6def27af","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2activationmap2.png\" width=\"500,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"9f37e048-c045-49dd-a067-16ae1cf80f9e","metadata":{},"outputs":[],"source":["Create a <code>Conv2d</code> with three channels:\n"]},{"cell_type":"code","id":"38fc3845-eb26-4559-b549-fa0b47baf4a3","metadata":{},"outputs":[],"source":["conv1 = nn.Conv2d(in_channels=1, out_channels=3,kernel_size=3)"]},{"cell_type":"markdown","id":"3d08bc53-271c-4bc0-bdfd-701fbb9767a3","metadata":{},"outputs":[],"source":["Pytorch randomly assigns values to each kernel. However, use kernels that have  been developed to detect edges:\n"]},{"cell_type":"code","id":"d171400d-3ca9-40c8-8696-e0cc021e7b00","metadata":{},"outputs":[],"source":["Gx=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\nGy=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])\n\nconv1.state_dict()['weight'][0][0]=Gx\nconv1.state_dict()['weight'][1][0]=Gy\nconv1.state_dict()['weight'][2][0]=torch.ones(3,3)"]},{"cell_type":"markdown","id":"46a6fe6e-4b0a-4031-ae42-d7553cd520e2","metadata":{},"outputs":[],"source":["Each kernel has its own bias, so set them all to zero:\n"]},{"cell_type":"code","id":"8315a2f7-b2e8-46d9-91b8-978ef0247de0","metadata":{},"outputs":[],"source":["conv1.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])\nconv1.state_dict()['bias']"]},{"cell_type":"markdown","id":"fa31353b-87d1-4ef0-a98c-42f4b9471243","metadata":{},"outputs":[],"source":["Print out each kernel: \n"]},{"cell_type":"code","id":"cd38f9a0-987e-4886-aaa9-134b2ee245ff","metadata":{},"outputs":[],"source":["for x in conv1.state_dict()['weight']:\n    print(x)"]},{"cell_type":"markdown","id":"ba4d26fb-1582-4841-bbfc-72c5d1d19dae","metadata":{},"outputs":[],"source":["Create an input <code>image</code> to represent the input X:\n"]},{"cell_type":"code","id":"7f340ad7-4c4d-4009-aa0f-8c6099580cbf","metadata":{},"outputs":[],"source":["image=torch.zeros(1,1,5,5)\nimage[0,0,:,2]=1\nimage"]},{"cell_type":"markdown","id":"0559693f-8b5c-47f1-9303-d0c5155270c8","metadata":{},"outputs":[],"source":["Plot it as an image: \n"]},{"cell_type":"code","id":"bdb29dbe-3a6d-4436-9add-38f843cfe2d7","metadata":{},"outputs":[],"source":["plt.imshow(image[0,0,:,:].numpy(), interpolation='nearest', cmap=plt.cm.gray)\nplt.colorbar()\nplt.show()"]},{"cell_type":"markdown","id":"444f7143-e59f-4780-a8e6-3e245ac74953","metadata":{},"outputs":[],"source":["Perform convolution using each channel: \n"]},{"cell_type":"code","id":"33dd432a-aeda-437a-b339-1e71fb6f859f","metadata":{},"outputs":[],"source":["out=conv1(image)"]},{"cell_type":"markdown","id":"a1ab76d8-48bf-44c1-832f-c669203239da","metadata":{},"outputs":[],"source":["The result is a 1x3x3x3 tensor. This represents one sample with three channels, and each channel contains a 3x3 image.  The same rules that govern the shape of each image were discussed in the last section.\n"]},{"cell_type":"code","id":"c557226a-e1ac-45ce-a214-d0d6376e1791","metadata":{},"outputs":[],"source":["out.shape"]},{"cell_type":"markdown","id":"0ede323c-60e0-4c82-8b04-7bac032701c8","metadata":{},"outputs":[],"source":["Print out each channel as a tensor or an image: \n"]},{"cell_type":"code","id":"c8230c74-d623-4e9c-bdd2-6a8477560f76","metadata":{},"outputs":[],"source":["for channel,image in enumerate(out[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()"]},{"cell_type":"markdown","id":"47f25bea-dc91-4ae9-a4a5-b83b8a5e2280","metadata":{},"outputs":[],"source":["Different kernels can be used to detect various features in an image. You can see that the first channel fluctuates, and the second two channels produce a constant value. The following figure summarizes the process:\n"]},{"cell_type":"markdown","id":"a1db9045-f7d8-4be2-9fba-c5594acf6b4e","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2outputsgray.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"199f1118-1b11-406e-8d67-94ed20b1cb7a","metadata":{},"outputs":[],"source":["If you use a different image, the result will be different: \n"]},{"cell_type":"code","id":"0843cc0f-e9a8-4a83-9190-03564f09f3d8","metadata":{},"outputs":[],"source":["image1=torch.zeros(1,1,5,5)\nimage1[0,0,2,:]=1\nprint(image1)\nplt.imshow(image1[0,0,:,:].detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\nplt.show()"]},{"cell_type":"markdown","id":"05677b9b-68c5-4055-bb09-28c039ddf1a0","metadata":{},"outputs":[],"source":["In this case, the second channel fluctuates, and the first and the third channels produce a constant value.\n"]},{"cell_type":"code","id":"0078fe47-844e-49d8-9cbc-f2dd1e8a831d","metadata":{},"outputs":[],"source":["out1=conv1(image1)\nfor channel,image in enumerate(out1[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()"]},{"cell_type":"markdown","id":"a15920e7-9462-4868-865b-59c14f8f1d7d","metadata":{},"outputs":[],"source":["The following figure summarizes the process:\n"]},{"cell_type":"markdown","id":"28a54080-67e1-4187-b7e6-f916dd14b865","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2ouputsgray2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"7f1fd998-e795-4695-961d-4e5b5a175a62","metadata":{},"outputs":[],"source":["<a id=\"ref1\"></a>\n","<h2 align=center>Multiple Input Channels </h2>\n"]},{"cell_type":"markdown","id":"0143e662-cedf-43b9-9949-0f251c67fdca","metadata":{},"outputs":[],"source":["For two inputs, you can create two kernels. Each kernel performs a convolution on its associated input channel. The resulting output is added together as shown:  \n"]},{"cell_type":"markdown","id":"02583c7b-8109-4a5e-9509-291f58817427","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.22chanalsinput.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"de480458-79ea-47ab-99f5-bd657d6988b2","metadata":{},"outputs":[],"source":["Create an input with two channels:\n"]},{"cell_type":"code","id":"41405eb8-2012-4786-bc08-579130612489","metadata":{},"outputs":[],"source":["image2=torch.zeros(1,2,5,5)\nimage2[0,0,2,:]=-2\nimage2[0,1,2,:]=1\nimage2"]},{"cell_type":"markdown","id":"50364b74-c64e-4082-b663-59ed25f2b755","metadata":{},"outputs":[],"source":["Plot out each image: \n"]},{"cell_type":"code","id":"c4094e83-58bf-4268-b2ad-16bb88ae83e1","metadata":{},"outputs":[],"source":["for channel,image in enumerate(image2[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()"]},{"cell_type":"markdown","id":"7bf54d87-2a64-49a9-b94f-46f3ad60336b","metadata":{},"outputs":[],"source":["Create a <code>Conv2d</code> object with two inputs:\n"]},{"cell_type":"code","id":"3a625ad9-b969-41ab-a799-3b9002edd3da","metadata":{},"outputs":[],"source":["conv3 = nn.Conv2d(in_channels=2, out_channels=1,kernel_size=3)"]},{"cell_type":"markdown","id":"dd9d2e72-ecc3-49d0-9439-db1edadb42e0","metadata":{},"outputs":[],"source":["Assign kernel values to make the math a little easier: \n"]},{"cell_type":"code","id":"d4c8ff28-1975-4639-94d5-d53c270a07ce","metadata":{},"outputs":[],"source":["Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\nconv3.state_dict()['weight'][0][0]=1*Gx1\nconv3.state_dict()['weight'][0][1]=-2*Gx1\nconv3.state_dict()['bias'][:]=torch.tensor([0.0])"]},{"cell_type":"code","id":"b25bc27e-6088-40bc-9e1d-d5e60d36a45a","metadata":{},"outputs":[],"source":["conv3.state_dict()['weight']"]},{"cell_type":"markdown","id":"2da2032e-5b7c-49ed-82e5-6cf73c91c96e","metadata":{},"outputs":[],"source":["Perform the convolution:\n"]},{"cell_type":"code","id":"25ed9a3b-f3a6-43c6-ac30-c9f64999ffd2","metadata":{},"outputs":[],"source":["conv3(image2)"]},{"cell_type":"markdown","id":"3ac31eaf-4e5f-4b7b-85a0-2f7058b0e053","metadata":{},"outputs":[],"source":["The following images summarize the process. The object performs Convolution.\n"]},{"cell_type":"markdown","id":"23d77939-1e68-4630-9c22-26167f81ac55","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_two_channal_example.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"5b809468-9305-40e2-b627-91f675614949","metadata":{},"outputs":[],"source":["Then, it adds the result: \n"]},{"cell_type":"markdown","id":"77200b8a-fbfd-45df-b072-4b832baab70d","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_two_channal_example2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"d688ca2a-199e-4a33-9076-931f44bea0ab","metadata":{},"outputs":[],"source":["<a id=\"ref2\"></a>\n","\n","<h2>Multiple Input and Multiple Output Channels</h2>\n"]},{"cell_type":"markdown","id":"e417124d-1d31-494d-9f05-396ab0d45813","metadata":{},"outputs":[],"source":["When using multiple inputs and outputs, a kernel is created for each input, and the process is repeated for each output. The process is summarized in the following image. \n","\n","There are two input channels and 3 output channels. For each channel, the input in red and purple is convolved with an individual kernel that is colored differently. As a result, there are three outputs. \n"]},{"cell_type":"markdown","id":"5202d903-42f1-409f-a22e-c7c131e68a47","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2mulit_input_output.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"d1ca5f7d-4640-4a21-bd55-99fce2d8bd33","metadata":{},"outputs":[],"source":["Create an example with two inputs and three outputs and assign the kernel values to make the math a little easier: \n"]},{"cell_type":"code","id":"369872cf-93cd-410a-a230-d88f3825b3a5","metadata":{},"outputs":[],"source":["conv4 = nn.Conv2d(in_channels=2, out_channels=3,kernel_size=3)\nconv4.state_dict()['weight'][0][0]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\nconv4.state_dict()['weight'][0][1]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\n\n\nconv4.state_dict()['weight'][1][0]=torch.tensor([[0.0,0.0,0.0],[0,1,0],[0.0,0.0,0.0]])\nconv4.state_dict()['weight'][1][1]=torch.tensor([[0.0,0.0,0.0],[0,-1,0],[0.0,0.0,0.0]])\n\nconv4.state_dict()['weight'][2][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\nconv4.state_dict()['weight'][2][1]=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])"]},{"cell_type":"markdown","id":"2c081c1a-c4b1-4202-bcb2-bd5eb8c8d242","metadata":{},"outputs":[],"source":["For each output, there is a bias, so set them all to zero: \n"]},{"cell_type":"code","id":"9e5ae986-2368-41b8-81fc-c5a00a152fa0","metadata":{},"outputs":[],"source":["conv4.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])"]},{"cell_type":"markdown","id":"f120d9e6-760c-4bda-ba8a-30299c886ca0","metadata":{},"outputs":[],"source":["Create a two-channel image and plot the results: \n"]},{"cell_type":"code","id":"f1a14a24-f53a-4b2a-9517-e9784ab47011","metadata":{},"outputs":[],"source":["image4=torch.zeros(1,2,5,5)\nimage4[0][0]=torch.ones(5,5)\nimage4[0][1][2][2]=1\nfor channel,image in enumerate(image4[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()"]},{"cell_type":"markdown","id":"8fc31849-bc84-4027-87d5-bbd6801729c6","metadata":{},"outputs":[],"source":["Perform the convolution:\n"]},{"cell_type":"code","id":"6c85f5bd-56c8-47c6-bc92-43bf5c99eb5f","metadata":{},"outputs":[],"source":["z=conv4(image4)\nz"]},{"cell_type":"markdown","id":"d714a7b2-cef3-4993-a02f-f9047b8392cd","metadata":{},"outputs":[],"source":["The output of the first channel is given by: \n"]},{"cell_type":"markdown","id":"8b2b50bd-5adc-422d-b61a-e04ee13b18a9","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_1.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"adf63f17-2180-4d7c-aeb7-81b5da23ceae","metadata":{},"outputs":[],"source":["The output of the second channel is given by:\n"]},{"cell_type":"markdown","id":"d163e66c-2f24-4f8e-adce-7ea831affd54","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"0da3f22a-8b66-4e57-917a-16ae2d3ecbf0","metadata":{},"outputs":[],"source":["The output of the third channel is given by: \n"]},{"cell_type":"markdown","id":"6dac87d0-87d4-4d29-bb34-26d3432e3a56","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_3.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"1027eecd-031e-4628-9ae7-9343c4bbc81b","metadata":{},"outputs":[],"source":["<a id=\"ref3\"></a>\n","\n","<h2>Practice Questions </h2>\n"]},{"cell_type":"markdown","id":"eb3ee453-3519-4d94-aa71-c4df7c2ebb9b","metadata":{},"outputs":[],"source":["Use the following two convolution objects to produce the same result as two input channel convolution on imageA and imageB as shown in the following image:\n"]},{"cell_type":"code","id":"f33ef3fa-0469-43bc-aa60-387612ce1c9b","metadata":{},"outputs":[],"source":["imageA=torch.zeros(1,1,5,5)\nimageB=torch.zeros(1,1,5,5)\nimageA[0,0,2,:]=-2\nimageB[0,0,2,:]=1\n\n\nconv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\nconv6 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n\n\nGx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\nconv5.state_dict()['weight'][0][0]=1*Gx1\nconv6.state_dict()['weight'][0][0]=-2*Gx1\nconv5.state_dict()['bias'][:]=torch.tensor([0.0])\nconv6.state_dict()['bias'][:]=torch.tensor([0.0])"]},{"cell_type":"markdown","id":"d900cdb2-8f73-46aa-b979-c79f7f916f8f","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2Practice%20Questions_1.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"c06eb806-8bdd-48cc-8587-b1ad28c9f93d","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2Practice%20Questions_2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"2c5fdea4-6fcb-4e1d-9044-5f19cd02eecd","metadata":{},"outputs":[],"source":["Double-click __here__ for the solution.\n","\n","<!-- Your answer is below:\n","conv5(imageA)+conv6(imageB)\n","-->\n","\n","\n"]},{"cell_type":"markdown","id":"76588c32-e583-4a2b-a443-922e29299115","metadata":{},"outputs":[],"source":["\n","<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n"]},{"cell_type":"markdown","id":"3f71ed16-0592-4a34-bde2-f7a073613125","metadata":{},"outputs":[],"source":["### About the Authors:  \n","[Joseph Santarcangelo]( https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n","\n","Other contributors: [Michelle Carey](  https://www.linkedin.com/in/michelleccarey/), [Mavis Zhou](  https://www.linkedin.com/in/jiahui-mavis-zhou-a4537814a/) \n"]},{"cell_type":"markdown","id":"85492e59-46c5-48ef-ab95-17474639ba77","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","\n","\n","<hr>\n","-->\n","\n","## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}