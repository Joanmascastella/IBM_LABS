{"cells":[{"cell_type":"markdown","id":"f0f0a89a-748a-42a1-ac83-821107e41e38","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","<h1 align=center><font size = 5>Convolutional Neural Network Simple example </font></h1> \n"]},{"cell_type":"markdown","id":"ea3ad7cc-a450-4960-9f78-e31a18921823","metadata":{},"outputs":[],"source":["\n","<h3>Objective for this Notebook<h3>    \n","<h5> 1. Learn Convolutional Neural Network</h5>\n","<h5> 2. Define Softmax , Criterion function, Optimizer and Train the  Model</h5>    \n","\n"]},{"cell_type":"markdown","id":"7e35fe5d-b2a8-4512-bfc6-15f229ae0fd5","metadata":{},"outputs":[],"source":["\n","# Table of Contents\n","In this lab, we will use a Convolutional Neural Networks to classify horizontal an vertical Lines \n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","<li><a href=\"#ref0\">Helper functions </a></li>\n","<li><a href=\"#ref1\"> Prepare Data </a></li>\n","<li><a href=\"#ref2\">Convolutional Neural Network </a></li>\n","<li><a href=\"#ref3\">Define Softmax , Criterion function, Optimizer and Train the  Model</a></li>\n","<li><a href=\"#ref4\">Analyse Results</a></li>\n","\n","<br>\n","<p></p>\n","Estimated Time Needed: <strong>25 min</strong>\n","</div>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"fc820316-4244-4f18-8bcd-eae298021d1f","metadata":{},"outputs":[],"source":["<a id=\"ref0\"></a>\n","<a name=\"ref0\"><h2 align=center>Helper functions </h2></a>\n"]},{"cell_type":"code","id":"6853cdbd-f4c6-4741-912e-840e9514ba8d","metadata":{},"outputs":[],"source":["import torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd"]},{"cell_type":"code","id":"144bcea9-62e7-4595-9bd3-7bffbf614856","metadata":{},"outputs":[],"source":["torch.manual_seed(4)"]},{"cell_type":"markdown","id":"e13efa49-fd53-439a-939d-24117ec5aafe","metadata":{},"outputs":[],"source":["function to plot out the parameters of the Convolutional layers  \n"]},{"cell_type":"code","id":"59431a6c-1ca4-4e2f-b6fc-a36fc49ec91c","metadata":{},"outputs":[],"source":["def plot_channels(W):\n    #number of output channels \n    n_out=W.shape[0]\n    #number of input channels \n    n_in=W.shape[1]\n    w_min=W.min().item()\n    w_max=W.max().item()\n    fig, axes = plt.subplots(n_out,n_in)\n    fig.subplots_adjust(hspace = 0.1)\n    out_index=0\n    in_index=0\n    #plot outputs as rows inputs as columns \n    for ax in axes.flat:\n    \n        if in_index>n_in-1:\n            out_index=out_index+1\n            in_index=0\n              \n        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        in_index=in_index+1\n\n    plt.show()"]},{"cell_type":"markdown","id":"6d1f9b05-dcd6-4a6a-9f32-640d440df07e","metadata":{},"outputs":[],"source":["<code>show_data</code>: plot out data sample\n"]},{"cell_type":"code","id":"f077b58f-3301-4b4b-be4c-a9048b17cf2e","metadata":{},"outputs":[],"source":["def show_data(dataset,sample):\n\n    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n    plt.title('y='+str(dataset.y[sample].item()))\n    plt.show()"]},{"cell_type":"markdown","id":"27c86ee2-c88a-48d1-a543-db9cff5c962d","metadata":{},"outputs":[],"source":["create some toy data \n"]},{"cell_type":"code","id":"b05268fe-4796-40b7-9463-1a4e56e9782d","metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\nclass Data(Dataset):\n    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n        \"\"\"\n        p:portability that pixel is wight  \n        N_images:number of images \n        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n        \"\"\"\n        if train==True:\n            np.random.seed(1)  \n        \n        #make images multiple of 3 \n        N_images=2*(N_images//2)\n        images=np.zeros((N_images,1,11,11))\n        start1=3\n        start2=1\n        self.y=torch.zeros(N_images).type(torch.long)\n\n        for n in range(N_images):\n            if offset>0:\n        \n                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n            else:\n                low=4\n                high=1\n        \n            if n<=N_images//2:\n                self.y[n]=0\n                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n            elif  n>N_images//2:\n                self.y[n]=1\n                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n           \n        \n        \n        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n        self.len=self.x.shape[0]\n        del(images)\n        np.random.seed(0)\n    def __getitem__(self,index):      \n        return self.x[index],self.y[index]\n    def __len__(self):\n        return self.len"]},{"cell_type":"markdown","id":"22ff935d-e287-4068-9f58-da83f9faf32c","metadata":{},"outputs":[],"source":["<code>plot_activation</code>: plot out the activations of the Convolutional layers  \n"]},{"cell_type":"code","id":"4e9c2d7e-b2ed-4db0-9b2e-ab234f78798b","metadata":{},"outputs":[],"source":["def plot_activations(A,number_rows= 1,name=\"\"):\n    A=A[0,:,:,:].detach().numpy()\n    n_activations=A.shape[0]\n    \n    \n    print(n_activations)\n    A_min=A.min().item()\n    A_max=A.max().item()\n\n    if n_activations==1:\n\n        # Plot the image.\n        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n\n    else:\n        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n        fig.subplots_adjust(hspace = 0.4)\n        for i,ax in enumerate(axes.flat):\n            if i< n_activations:\n                # Set the label for the sub-plot.\n                ax.set_xlabel( \"activation:{0}\".format(i+1))\n\n                # Plot the image.\n                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n                ax.set_xticks([])\n                ax.set_yticks([])\n    plt.show()"]},{"cell_type":"markdown","id":"a4d3b4f8-ca51-458c-ab55-eb4033d96117","metadata":{},"outputs":[],"source":["\n","Utility function for computing output of convolutions\n","takes a tuple of (h,w) and returns a tuple of (h,w)\n"]},{"cell_type":"code","id":"96fb700b-207b-49c2-9d31-220d3c8a77b0","metadata":{},"outputs":[],"source":["\ndef conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n    #by Duane Nielsen\n    from math import floor\n    if type(kernel_size) is not tuple:\n        kernel_size = (kernel_size, kernel_size)\n    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n    return h, w"]},{"cell_type":"markdown","id":"67b19b3a-92db-4ee4-b86c-4a7824445ec2","metadata":{},"outputs":[],"source":["<a id=\"ref1\"></a>\n","<a name=\"ref1\"><h2 align=center>Prepare Data </h2></a>\n"]},{"cell_type":"markdown","id":"c592a7a1-3eb0-42ac-9296-faf52ed115fe","metadata":{},"outputs":[],"source":["Load the training dataset with 10000 samples \n"]},{"cell_type":"code","id":"b96fcb77-b63e-4341-8c36-c5828d510f0f","metadata":{},"outputs":[],"source":["N_images=10000\ntrain_dataset=Data(N_images=N_images)"]},{"cell_type":"markdown","id":"77614c7f-84b0-4d0a-89ee-707388213825","metadata":{},"outputs":[],"source":["Load the testing dataset\n"]},{"cell_type":"code","id":"60fafb39-6b6f-48a8-904f-3a89576ecc17","metadata":{},"outputs":[],"source":["validation_dataset=Data(N_images=1000,train=False)\nvalidation_dataset"]},{"cell_type":"markdown","id":"26617735-deee-425a-b94a-c8ed2827e7ca","metadata":{},"outputs":[],"source":["we can see the data type is long \n"]},{"cell_type":"markdown","id":"58044588-3e6b-4c1a-b9d2-643402d23d64","metadata":{},"outputs":[],"source":["### Data Visualization \n"]},{"cell_type":"markdown","id":"8de22d7a-6a99-4262-9b84-823812bca879","metadata":{},"outputs":[],"source":["Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"]},{"cell_type":"markdown","id":"883dd5b8-8198-4970-93b7-022fde466f9d","metadata":{},"outputs":[],"source":["We can print out the third label \n"]},{"cell_type":"code","id":"b543b547-a8d1-44bb-8caf-6874eae35091","metadata":{},"outputs":[],"source":["show_data(train_dataset,0)"]},{"cell_type":"code","id":"f36e68f0-a1ff-485e-998c-c429ee735e0d","metadata":{},"outputs":[],"source":["show_data(train_dataset,N_images//2+2)"]},{"cell_type":"markdown","id":"f94c519e-af9f-4473-927e-c9d7a94f4487","metadata":{},"outputs":[],"source":["we can plot the 3rd  sample \n"]},{"cell_type":"markdown","id":"061568f4-3729-401c-a612-c2e6f224067c","metadata":{},"outputs":[],"source":["<a id=\"ref3\"></a>\n","### Build a Convolutional Neral Network Class \n"]},{"cell_type":"markdown","id":"6f05b82c-0494-4955-b932-1414159b7f05","metadata":{},"outputs":[],"source":["The input image is 11 x11, the following will change the size of the activations:\n","<ul>\n","<il>convolutional layer</il> \n","</ul>\n","<ul>\n","<il>max pooling layer</il> \n","</ul>\n","<ul>\n","<il>convolutional layer </il>\n","</ul>\n","<ul>\n","<il>max pooling layer </il>\n","</ul>\n","\n","with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\n","We use the following  lines of code to change the image before we get tot he fully connected layer \n"]},{"cell_type":"code","id":"7b27e976-8211-48d9-817e-e662ed7d2356","metadata":{},"outputs":[],"source":["out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out)\nout1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out1)\nout2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out2)\n\nout3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out3)"]},{"cell_type":"markdown","id":"761b8f78-d845-462a-9d99-6e60e3ac750a","metadata":{},"outputs":[],"source":["Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"]},{"cell_type":"code","id":"29a17276-5545-454f-9c23-8f41279f5d11","metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n    def __init__(self,out_1=2,out_2=1):\n        \n        super(CNN,self).__init__()\n        #first Convolutional layers \n        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n\n        #second Convolutional layers\n        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n        #max pooling \n\n        #fully connected layer \n        self.fc1=nn.Linear(out_2*7*7,2)\n        \n    def forward(self,x):\n        #first Convolutional layers\n        x=self.cnn1(x)\n        #activation function \n        x=torch.relu(x)\n        #max pooling \n        x=self.maxpool1(x)\n        #first Convolutional layers\n        x=self.cnn2(x)\n        #activation function\n        x=torch.relu(x)\n        #max pooling\n        x=self.maxpool2(x)\n        #flatten output \n        x=x.view(x.size(0),-1)\n        #fully connected layer\n        x=self.fc1(x)\n        return x\n    \n    def activations(self,x):\n        #outputs activation this is not necessary just for fun \n        z1=self.cnn1(x)\n        a1=torch.relu(z1)\n        out=self.maxpool1(a1)\n        \n        z2=self.cnn2(out)\n        a2=torch.relu(z2)\n        out=self.maxpool2(a2)\n        out=out.view(out.size(0),-1)\n        return z1,a1,z2,a2,out        "]},{"cell_type":"markdown","id":"efe1b343-d0a5-4a95-a199-69a9f89d3ace","metadata":{},"outputs":[],"source":["<a id=\"ref3\"></a>\n","<li><a name=\"ref3\"><h2> Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model  </h2></a> \n"]},{"cell_type":"markdown","id":"44605c34-7b28-47a6-a0f8-5d6cd605c15f","metadata":{},"outputs":[],"source":["There are 2 output channels for the first layer, and 1 outputs channel for the second layer \n"]},{"cell_type":"code","id":"9c217d52-e740-4561-9f78-d1a4f60df17f","metadata":{},"outputs":[],"source":["model=CNN(2,1)"]},{"cell_type":"markdown","id":"52270f8d-add2-4468-a8fb-da25f2750564","metadata":{},"outputs":[],"source":["we can see the model parameters with the object \n"]},{"cell_type":"code","id":"f72a60bb-8294-4d08-a4c6-bcc52387b2ea","metadata":{},"outputs":[],"source":["model"]},{"cell_type":"markdown","id":"b4963868-e587-41f6-959d-9446d6590208","metadata":{},"outputs":[],"source":["Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"]},{"cell_type":"code","id":"5a01ee5d-e243-4b3e-bfb5-eb331ed961b2","metadata":{},"outputs":[],"source":["\nplot_channels(model.state_dict()['cnn1.weight'])\n"]},{"cell_type":"markdown","id":"7bb0f40e-7a67-4cbc-a150-931cf597844f","metadata":{},"outputs":[],"source":["Loss function \n"]},{"cell_type":"code","id":"827b8a6e-48f4-4285-859d-c30b663b7fa2","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","id":"1e9bdeed-5336-46c6-8258-c521a1bdd605","metadata":{},"outputs":[],"source":["Define the loss function \n"]},{"cell_type":"code","id":"bd7e1fec-a3c3-4a6f-a07c-dd5b7ee479c6","metadata":{},"outputs":[],"source":["criterion=nn.CrossEntropyLoss()"]},{"cell_type":"markdown","id":"1099c373-8f14-4b9b-9e7d-c23e4c657784","metadata":{},"outputs":[],"source":[" optimizer class \n"]},{"cell_type":"code","id":"51412994-903b-433e-8bc3-0787da4f6e6c","metadata":{},"outputs":[],"source":["learning_rate=0.001\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","id":"18e17ed3-f12c-4cf7-95ce-6fcdabb34fd6","metadata":{},"outputs":[],"source":["Define the optimizer class \n"]},{"cell_type":"code","id":"9c94623b-6bef-4055-863f-994037d57cd1","metadata":{},"outputs":[],"source":["\ntrain_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\nvalidation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"]},{"cell_type":"markdown","id":"10220586-57bd-4a2f-abf3-3f0d3e2bb644","metadata":{},"outputs":[],"source":["Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"]},{"cell_type":"code","id":"53277a19-69b9-43c4-891b-b62c2bb0cb79","metadata":{},"outputs":[],"source":["n_epochs=10\ncost_list=[]\naccuracy_list=[]\nN_test=len(validation_dataset)\ncost=0\n#n_epochs\nfor epoch in range(n_epochs):\n    cost=0    \n    for x, y in train_loader:\n      \n\n        #clear gradient \n        optimizer.zero_grad()\n        #make a prediction \n        z=model(x)\n        # calculate loss \n        loss=criterion(z,y)\n        # calculate gradients of parameters \n        loss.backward()\n        # update parameters \n        optimizer.step()\n        cost+=loss.item()\n    cost_list.append(cost)\n        \n        \n    correct=0\n    #perform a prediction on the validation  data  \n    for x_test, y_test in validation_loader:\n\n        z=model(x_test)\n        _,yhat=torch.max(z.data,1)\n\n        correct+=(yhat==y_test).sum().item()\n        \n\n    accuracy=correct/N_test\n\n    accuracy_list.append(accuracy)\n    \n\n"]},{"cell_type":"markdown","id":"7fb20929-6ba6-4fdf-a609-66df1cf7646d","metadata":{},"outputs":[],"source":["#### <a id=\"ref4\"></a>\n","<li><a name=\"ref4\"><h2 align=center>Analyse Results</h2></a>\n"]},{"cell_type":"markdown","id":"7782be30-5f3b-4777-9aa5-994eef95926f","metadata":{},"outputs":[],"source":["Plot the loss and accuracy on the validation data:\n"]},{"cell_type":"code","id":"16364d0c-2547-41ac-9dc1-c93da70fb0c9","metadata":{},"outputs":[],"source":["fig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(cost_list,color=color)\nax1.set_xlabel('epoch',color=color)\nax1.set_ylabel('total loss',color=color)\nax1.tick_params(axis='y', color=color)\n    \nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color)  \nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', labelcolor=color)\nfig.tight_layout()"]},{"cell_type":"markdown","id":"529b6255-0ff9-455b-b1ec-7d375d995936","metadata":{},"outputs":[],"source":["View the results of the parameters for the Convolutional layers \n"]},{"cell_type":"code","id":"c39eeec5-26b8-4e0a-abf8-cc647cebaf94","metadata":{},"outputs":[],"source":["model.state_dict()['cnn1.weight']"]},{"cell_type":"code","id":"670e9aab-2397-49c9-a503-42b31d82f8d3","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn1.weight'])"]},{"cell_type":"code","id":"01e475aa-cfdb-4a7e-9bd5-68b966ab0174","metadata":{},"outputs":[],"source":["model.state_dict()['cnn1.weight']"]},{"cell_type":"code","id":"e32e1c7c-3d3c-46f8-9d1e-2f462e812aa8","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","id":"ef58b9d4-d8c0-4195-b588-f1e55a9f8254","metadata":{},"outputs":[],"source":["Consider the following sample \n"]},{"cell_type":"code","id":"3c157332-a959-4f69-9c29-666cea3f4ff1","metadata":{},"outputs":[],"source":["show_data(train_dataset,N_images//2+2)"]},{"cell_type":"markdown","id":"2129772a-ce7c-4ae3-9de2-45a49f86b1b5","metadata":{},"outputs":[],"source":["Determine the activations \n"]},{"cell_type":"code","id":"c0269e40-8c13-4bee-a5bd-8d8b4087f4e5","metadata":{},"outputs":[],"source":["out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\nout=model.activations(train_dataset[0][0].view(1,1,11,11))"]},{"cell_type":"markdown","id":"a0fe5a7a-ff89-4455-8af0-4e9d5c89608a","metadata":{},"outputs":[],"source":["Plot them out\n"]},{"cell_type":"code","id":"cd4e1b9d-3d7c-450f-81a2-cc93bcb94dee","metadata":{},"outputs":[],"source":["plot_activations(out[0],number_rows=1,name=\" feature map\")\nplt.show()\n"]},{"cell_type":"code","id":"c4d1eac2-2fe6-4eff-9df9-a75de503b1c9","metadata":{},"outputs":[],"source":["plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\nplt.show()"]},{"cell_type":"code","id":"26ed4b1b-32b3-45c5-94d1-7bd0c64f3242","metadata":{},"outputs":[],"source":["plot_activations(out[3],number_rows=1,name=\"first feature map\")\nplt.show()"]},{"cell_type":"markdown","id":"9077136d-88a2-4444-9d7b-65e9c3f6593f","metadata":{},"outputs":[],"source":["we save the output of the activation after flattening  \n"]},{"cell_type":"code","id":"1dddfdfd-d0eb-4fd7-88c0-5e6bade014ad","metadata":{},"outputs":[],"source":["out1=out[4][0].detach().numpy()"]},{"cell_type":"markdown","id":"e295d70d-4ee0-430f-9935-ec5c22a60e65","metadata":{},"outputs":[],"source":["we can do the same for a sample  where y=0 \n"]},{"cell_type":"code","id":"7a0d7d21-4da7-4f0a-a785-e8f8df0c47b2","metadata":{},"outputs":[],"source":["out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\nout0"]},{"cell_type":"code","id":"26b8de8f-5967-455d-bf03-1e9fe0b672fb","metadata":{},"outputs":[],"source":["plt.subplot(2, 1, 1)\nplt.plot( out1, 'b')\nplt.title('Flatted Activation Values  ')\nplt.ylabel('Activation')\nplt.xlabel('index')\nplt.subplot(2, 1, 2)\nplt.plot(out0, 'r')\nplt.xlabel('index')\nplt.ylabel('Activation')"]},{"cell_type":"markdown","id":"fe5713a2-da42-4f99-9564-d53c698919c0","metadata":{},"outputs":[],"source":["\n","\n","<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n","\n"]},{"cell_type":"markdown","id":"adcf91a9-3638-4b07-aa11-5d3cb2ff9b62","metadata":{},"outputs":[],"source":["### About the Authors:  \n","[Joseph Santarcangelo]( https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n","\n","Other contributors: [Michelle Carey](  https://www.linkedin.com/in/michelleccarey/) \n"]},{"cell_type":"markdown","id":"bd6f426e-742e-44a6-be6a-074f06e7d936","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","\n","\n","<hr>\n","-->\n","\n","## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"prev_pub_hash":"ca37977d391a59f1fa2884c31403b72d67e02b6915b5c0d97a31f39cef1cb4e6"},"nbformat":4,"nbformat_minor":4}